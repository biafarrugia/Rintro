---
title: "Importing and exporting with rio"
output: html_notebook
---

# Importing and exporting with rio

The `rio` package "handles more than two dozen formats including tab-separated data (with the extension .tsv), JSON, Stata, and fixed-width format data (.fwf)."

```{r install rio}
#If rio is not installed, install it
if(!require("rio")){
  install.packages("rio")
}
library(rio)
```

Now, download the data to work with:

```{r download file}
download.file("http://bit.ly/BostonSnowfallCSV", "BostonWinterSnowfalls.csv")
```

## Importing the data

And import it from where is was saved, in the same folder as this script:

```{r import data}
snowdata <- rio::import("BostonWinterSnowfalls.csv")
```

Alternatively you can download and import all in one by specifying a URL. If the format isn't in the URL, as here, specify it with the `format =` parameter:

```{r import from url, eval=FALSE, include=FALSE}
snowdata <- rio::import("http://bit.ly/BostonSnowfallCSV", format = "csv")
```

And you can scrape a HTML table (as long as it's well formatted) in the same way:

```{r scrape html table}
rsi_description <- rio::import("https://www.ncdc.noaa.gov/snow-and-ice/rsi/", format="html")
```

Let's try it with another page:

```{r scrape trolleywatch}
trolleywatch <- rio::import("https://www.inmo.ie/Trolley_Ward_Watch", format="html")
head(trolleywatch)
```

Note that the column names aren't fetched. I suspect this is because they don't use the `<th>` tag.

What if the page has more than one HTML table? You can add `which = 2` to specify the second table, and so on. If this isn't specified, it will grab the first.

```{r scrape 2nd table}
#Show the parameters - includes which
?rio::import
#Scrape with that which parameter
anothertable <- rio::import("https://www.w3schools.com/html/html_tables.asp", format="html", which = 2)
```

You can use the [Chrome extension Table Capture](https://chrome.google.com/webstore/detail/table-capture/iebpjdmgckacbodjpijphcplhebcmeop?hl=en) to identify which table you want.

But unless the table is nice, it'll cause problems.

## Using `htmltab` to scrape tables instead

[The `htmltab` package](https://cran.r-project.org/web/packages/htmltab/vignettes/htmltab.html) is better suited for this task.

```{r install htmltab}
#Install htmltab if needed
if(!require(htmltab)){
  install.packages('htmltab')
}
#Then activate
library(htmltab)
```

The Bob Dylan discography page on Wikipedia causes problems for `rio`, but `htmltab` can handle it:

```{r scrape Wikipedia page}
dylanscrape <- htmltab::htmltab("https://en.wikipedia.org/wiki/Bob_Dylan_discography", which = 1)
#Check the data types
str(dylanscrape)
```

Note that the numbers are being treated as character strings here. We will install a package to deal with that...

## Installing a package from GitHub

There are various ways to install packages from GitHub - one is to install `pacman` first. Another is `remotes`, and a third is `githubinstall`.

```{r install pacman}
if(!require(pacman)){
  install.packages("pacman")
}
library(pacman)
```

The `p_load_gh()` function loads from a GitHub repo - it means you don't need to specify the whole URL: just the username and repo name

```{r load package from GitHub}
#Sharon's username is smach, and the repo for her package is rmiscutils
pacman::p_load_gh("smach/rmiscutils")
#Activate the library
library(rmiscutils)
```
Once loaded, we can use the `number_with_commas()` function from that library. But which column:

```{r check column names}
#Check the column names
colnames(dylanscrape)
```

We need to rename them:

```{r show head}
head(dylanscrape)
```

```{r rename columns}
colnames(dylanscrape) <- c("type","number")
head(dylanscrape)
```


```{r Use the function number_with_commas()}
#Use the function number_with_commas()
dylanscrape$numberclean <- rmiscutils::number_with_commas(dylanscrape$number)
#Check results
str(dylanscrape)
```

## Converting text to numbers using `readr`

The `readr` package was designed for this problem, too, with its various `parse` functions, including `parse_number()`:

```{r use parse_number}
#Use the function 
dylanscrape$numberclean2 <- readr::parse_number(dylanscrape$number)
#Check results
str(dylanscrape)
```

## Cleaning column names with `janitor`

The `janitor` package can be used to clean up column names that aren't suitable for R, such as those with spaces, starting with numbers, etc.

```{r install janitor}
if(!require(janitor)){
  install.packages("janitor")
}
library(janitor)
```

Now let's scrape a table with problematic headings

```{r scrape Wikipedia table}
citytable <- htmltab("https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population", which = 5)
colnames(citytable)
```

Then use the `clean_names()` function on that data frame:

```{r run janitor clean_names function}
citytable <- janitor::clean_names(citytable)
#Check the results
colnames(citytable)
```

Note that an x is placed in front of numbers, and spaces are replaced with underscores. Any capitals are made lower case.

```{r repeat a string}
rep("character", 5)
```

## Using the FiveThirtyEight package

[The FiveThirtyEight package](https://github.com/rudeboybert/fivethirtyeight) "was created by several academics in consultation with FiveThirtyEight editors, and is designed to be a resource for teaching undergraduate statistics."

```{r install fivethirtyeight}
#Install and activate
if(!require(fivethirtyeight)){
  pacman::p_load_gh("rudeboybert/fivethirtyeight")
}
library(fivethirtyeight)
#Show the datasets we can now access
data(package = "fivethirtyeight")
#Grab one of the datasets
data(biopics)
#And show the head
head(biopics)
#And data types
str(biopics)
```


